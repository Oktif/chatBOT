{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoheEdCzVvTo",
        "outputId": "6cd95006-5d75-4781-f99b-828dc50b7750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.8/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (2.25.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: translate in /usr/local/lib/python3.8/dist-packages (3.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from translate) (2.25.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from translate) (4.9.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from translate) (7.1.2)\n",
            "Requirement already satisfied: libretranslatepy==2.1.1 in /usr/local/lib/python3.8/dist-packages (from translate) (2.1.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "CHATBOT - final NAI project\n",
        "All needed info avalible in read.me or in presentation\n",
        "Programming team : Oktawian Filipkowski\n",
        "'''\n",
        "\n",
        "'''\n",
        "Required installs\n",
        "'''\n",
        "!pip install wikipedia\n",
        "!pip install translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFuisCAnWOsn",
        "outputId": "a2207f6d-87e3-4f3d-ad77-99a9503db46b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "Required imports\n",
        "'''\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from translate import Translator\n",
        "import random\n",
        "import wikipedia\n",
        "import nltk\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import random\n",
        "'''\n",
        "NLTK downloads\n",
        "'''\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NBkkHuzAWZfU"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Creating fucntions needed in wikipedia part of the bot\n",
        "'''\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemma(sent):\n",
        "    sentence_tokens = nltk.word_tokenize(sent.lower())\n",
        "    pos_tags = nltk.pos_tag(sentence_tokens)\n",
        "\n",
        "    sentence_lemmas = []\n",
        "    for token, pos_tag in zip(sentence_tokens, pos_tags):\n",
        "        if pos_tag[1][0].lower() in ['n', 'v', 'a', 'r']:\n",
        "            lemma = lemmatizer.lemmatize(token, pos_tag[1][0].lower())\n",
        "            sentence_lemmas.append(lemma)\n",
        "\n",
        "    return sentence_lemmas\n",
        "\n",
        "def process(text, question):\n",
        "  sentence_tokens = nltk.sent_tokenize(text)\n",
        "  sentence_tokens.append(question)\n",
        "\n",
        "  tv = TfidfVectorizer(tokenizer=lemma)\n",
        "  tf = tv.fit_transform(sentence_tokens)\n",
        "  values = cosine_similarity(tf[-1], tf)\n",
        "  index = values.argsort()[0][-2]\n",
        "  values_flat = values.flatten()\n",
        "  values_flat.sort()\n",
        "  coeff = values_flat[-2]\n",
        "  if coeff > 0.3:\n",
        "    return sentence_tokens[index]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Creating functions needed to translate\n",
        "'''\n",
        "def translate(translate):\n",
        "  translator= Translator(from_lang=\"english\",to_lang=\"polish\")\n",
        "  MyString = translate\n",
        "  i = len(MyString)\n",
        "  if i%2 == 0:\n",
        "      str1 = MyString[0:i//2]\n",
        "      str2 = MyString[i//2:]\n",
        "      sentence = str1\n",
        "      sentence2 = str2\n",
        "      translation = translator.translate(sentence)\n",
        "      translation2 = translator.translate(sentence2)\n",
        "      print(translation, translation2)\n",
        "  else:\n",
        "      str1 = MyString[0:(i//2+1)]\n",
        "      str2 = MyString[(i//2+1):]\n",
        "      sentence = str1\n",
        "      sentence2 = str2\n",
        "      translation = translator.translate(sentence)\n",
        "      translation2 = translator.translate(sentence2)\n",
        "      print(translation, translation2)\n",
        "\n",
        "def translate_eng(translate):\n",
        "  translator= Translator(from_lang=\"polish\",to_lang=\"english\")\n",
        "  MyString = translate\n",
        "  i = len(MyString)\n",
        "  if i%2 == 0:\n",
        "      str1 = MyString[0:i//2]\n",
        "      str2 = MyString[i//2:]\n",
        "      sentence = str1\n",
        "      sentence2 = str2\n",
        "      translation = translator.translate(sentence)\n",
        "      translation2 = translator.translate(sentence2)\n",
        "      translated = (translation +' '+ translation2)\n",
        "      return translated\n",
        "  else:\n",
        "      str1 = MyString[0:(i//2+1)]\n",
        "      str2 = MyString[(i//2+1):]\n",
        "      sentence = str1\n",
        "      sentence2 = str2\n",
        "      translation = translator.translate(sentence)\n",
        "      translation2 = translator.translate(sentence2)\n",
        "      translated = (translation +' '+ translation2)\n",
        "      return translated"
      ],
      "metadata": {
        "id": "3v6D_AlD8CeA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MXShA0BfalZN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Creating objects needed in learning part of the bot\n",
        "'''\n",
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open('intents.json').read()\n",
        "intents = json.loads(data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPq2g1tOZ-t6",
        "outputId": "c5e4186e-e6ee-4d42-96f6-aa2c95873b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105 documents\n",
            "43 classes ['Location', 'Weather', 'animals', 'anime', 'anime_reccomendation', 'birthday', 'cats', 'commission', 'configuration', 'connect_people', 'customer_satisfaction', 'dogs', 'email', 'factors_impacting_sale', 'feelings', 'gadgets', 'game_info', 'games', 'games_reccomendation', 'goodbye', 'greeting', 'highest_grossing', 'hours', 'invalid', 'key_customers', 'leave', 'maintainence', 'manga', 'music', 'name', 'noans', 'options', 'predict_delay', 'predict_performance', 'project_handling_queries', 'rock', 'search_department', 'solve_problems', 'thanks', 'turnover', 'vegetables', 'version_update', 'vitamins']\n",
            "184 unique lemmatized words [\"'s\", ',', 'a', 'about', 'adopt', 'aggresive', 'am', 'an', 'animal', 'anime', 'any', 'anyone', 'are', 'at', 'avalible', 'awesome', 'be', 'been', 'best', 'bored', 'born', 'break', 'bye', 'cafeteria', 'call', 'can', 'canteen', 'chat', 'chatting', 'commission', 'company', 'complaint', 'computer', 'configuration', 'configure', 'conflict', 'could', 'create', 'creation', 'customer', 'date', 'day', 'delayed', 'demand', 'department', 'desktop', 'do', 'doe', 'dog', 'each', 'email', 'employee', 'factor', 'fast', 'favorite', 'feedback', 'for', 'friend', 'gadget', 'game', 'gen', 'genre', 'give', 'good', 'goodbye', 'grossing', 'ha', 'happy', 'hardware', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'highest', 'hola', 'hour', 'how', 'i', 'im', 'impact', 'impacting', 'in', 'information', 'insufficient', 'interested', 'is', 'issue', 'key', 'know', 'laptop', 'later', 'like', 'listen', 'location', 'lost', 'love', 'ma', 'maintainence', 'management', 'manga', 'marry', 'me', 'meet', 'miscommunication', 'music', 'my', 'name', 'need', 'needed', 'next', 'nice', 'now', 'occured', 'of', 'office', 'on', 'open', 'opening', 'our', 'people', 'play', 'present', 'problem', 'product', 'provide', 'raised', 'rate', 'reccomend', 'recommendation', 'regarding', 'required', 'resolved', 'response', 'risk', 'sale', 'see', 'should', 'skill', 'skilled', 'software', 'solve', 'something', 'step', 'stock', 'target', 'team', 'tell', 'thank', 'thanks', 'that', 'the', 'there', 'think', 'this', 'till', 'time', 'to', 'today', 'track', 'turnover', 'update', 'urgently', 'various', 'vegetable', 'version', 'vitamin', 'wa', 'want', 'we', 'weather', 'were', 'what', 'when', 'where', 'which', 'who', 'why', 'with', 'year', 'you', 'your']\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Tokenization, adding docs to corpus, lemmatization.\n",
        "Classes will be counted and unique lemmatized words will be printed for reference\n",
        "'''\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        documents.append((w, intent['tag']))\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pKrgWGFbGIU",
        "outputId": "54fe6802-f317-401e-ef7f-e64a1a30f0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-43399ea6a252>:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Creation of training data and empty array\n",
        "'''\n",
        "training = []\n",
        "\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "    bag = []\n",
        "    pattern_words = doc[0]\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXDVdEDrbKN2",
        "outputId": "b9f9cc89-16ec-4eff-b8b0-30526911e863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "21/21 [==============================] - 1s 2ms/step - loss: 3.7813 - accuracy: 0.0095 \n",
            "Epoch 2/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 3.7190 - accuracy: 0.0381\n",
            "Epoch 3/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 3.6601 - accuracy: 0.0952\n",
            "Epoch 4/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 3.5939 - accuracy: 0.1143\n",
            "Epoch 5/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 3.4880 - accuracy: 0.1048\n",
            "Epoch 6/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 3.4155 - accuracy: 0.1238\n",
            "Epoch 7/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 3.3691 - accuracy: 0.1238\n",
            "Epoch 8/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 3.2175 - accuracy: 0.1333\n",
            "Epoch 9/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.1253 - accuracy: 0.1619\n",
            "Epoch 10/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 3.0230 - accuracy: 0.2571\n",
            "Epoch 11/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.9154 - accuracy: 0.2571\n",
            "Epoch 12/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.8522 - accuracy: 0.2762\n",
            "Epoch 13/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.7108 - accuracy: 0.2952\n",
            "Epoch 14/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.6368 - accuracy: 0.3238\n",
            "Epoch 15/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 2.3412 - accuracy: 0.4095\n",
            "Epoch 16/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.4039 - accuracy: 0.3333\n",
            "Epoch 17/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2140 - accuracy: 0.4190\n",
            "Epoch 18/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1752 - accuracy: 0.4190\n",
            "Epoch 19/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.0251 - accuracy: 0.5238\n",
            "Epoch 20/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 1.8992 - accuracy: 0.4857\n",
            "Epoch 21/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.9411 - accuracy: 0.4571\n",
            "Epoch 22/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 1.8382 - accuracy: 0.4857\n",
            "Epoch 23/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 1.5388 - accuracy: 0.5429\n",
            "Epoch 24/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.5357 - accuracy: 0.5524\n",
            "Epoch 25/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.5267 - accuracy: 0.6381\n",
            "Epoch 26/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.4480 - accuracy: 0.6286\n",
            "Epoch 27/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 1.3990 - accuracy: 0.6381\n",
            "Epoch 28/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 1.3588 - accuracy: 0.6286\n",
            "Epoch 29/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2926 - accuracy: 0.7048\n",
            "Epoch 30/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2538 - accuracy: 0.7048\n",
            "Epoch 31/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.1304 - accuracy: 0.6857\n",
            "Epoch 32/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.1428 - accuracy: 0.6667\n",
            "Epoch 33/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.0688 - accuracy: 0.7619\n",
            "Epoch 34/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.0502 - accuracy: 0.7524\n",
            "Epoch 35/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 1.0093 - accuracy: 0.6952\n",
            "Epoch 36/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.7930 - accuracy: 0.7905\n",
            "Epoch 37/200\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.9972 - accuracy: 0.7429\n",
            "Epoch 38/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 1.0315 - accuracy: 0.6571\n",
            "Epoch 39/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.9529 - accuracy: 0.7333\n",
            "Epoch 40/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.8030 - accuracy: 0.7524\n",
            "Epoch 41/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.8537 - accuracy: 0.7619\n",
            "Epoch 42/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.9349 - accuracy: 0.7143\n",
            "Epoch 43/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.8397 - accuracy: 0.7810\n",
            "Epoch 44/200\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.7114 - accuracy: 0.8190\n",
            "Epoch 45/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.7504 - accuracy: 0.7714\n",
            "Epoch 46/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.6219 - accuracy: 0.8095\n",
            "Epoch 47/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6233 - accuracy: 0.8000\n",
            "Epoch 48/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.7274 - accuracy: 0.7905\n",
            "Epoch 49/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.8286\n",
            "Epoch 50/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.7847 - accuracy: 0.7238\n",
            "Epoch 51/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6945 - accuracy: 0.8190\n",
            "Epoch 52/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6478 - accuracy: 0.8381\n",
            "Epoch 53/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6267 - accuracy: 0.8095\n",
            "Epoch 54/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.5929 - accuracy: 0.7810\n",
            "Epoch 55/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.5844 - accuracy: 0.8095\n",
            "Epoch 56/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.7263 - accuracy: 0.7810\n",
            "Epoch 57/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4896 - accuracy: 0.8476\n",
            "Epoch 58/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6294 - accuracy: 0.7810\n",
            "Epoch 59/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6750 - accuracy: 0.8000\n",
            "Epoch 60/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.5758 - accuracy: 0.8286\n",
            "Epoch 61/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.5857 - accuracy: 0.8095\n",
            "Epoch 62/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.4423 - accuracy: 0.8571\n",
            "Epoch 63/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4461 - accuracy: 0.8857\n",
            "Epoch 64/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.5513 - accuracy: 0.8190\n",
            "Epoch 65/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.5158 - accuracy: 0.8476\n",
            "Epoch 66/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3390 - accuracy: 0.9238\n",
            "Epoch 67/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.4388 - accuracy: 0.8762\n",
            "Epoch 68/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3957 - accuracy: 0.8952\n",
            "Epoch 69/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4674 - accuracy: 0.8667\n",
            "Epoch 70/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.4208 - accuracy: 0.8857\n",
            "Epoch 71/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.4153 - accuracy: 0.8667\n",
            "Epoch 72/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3456 - accuracy: 0.8762\n",
            "Epoch 73/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.3178 - accuracy: 0.8952\n",
            "Epoch 74/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3809 - accuracy: 0.8857\n",
            "Epoch 75/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.3682 - accuracy: 0.8857\n",
            "Epoch 76/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.5313 - accuracy: 0.8476\n",
            "Epoch 77/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.3182 - accuracy: 0.9048\n",
            "Epoch 78/200\n",
            "21/21 [==============================] - 0s 10ms/step - loss: 0.3432 - accuracy: 0.8952\n",
            "Epoch 79/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.3791 - accuracy: 0.8952\n",
            "Epoch 80/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4217 - accuracy: 0.8476\n",
            "Epoch 81/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3973 - accuracy: 0.8667\n",
            "Epoch 82/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2854 - accuracy: 0.9048\n",
            "Epoch 83/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.8476\n",
            "Epoch 84/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8667\n",
            "Epoch 85/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3748 - accuracy: 0.8667\n",
            "Epoch 86/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3850 - accuracy: 0.8762\n",
            "Epoch 87/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3763 - accuracy: 0.8762\n",
            "Epoch 88/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3446 - accuracy: 0.8952\n",
            "Epoch 89/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2817 - accuracy: 0.9238\n",
            "Epoch 90/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2993 - accuracy: 0.9333\n",
            "Epoch 91/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2917 - accuracy: 0.9048\n",
            "Epoch 92/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3353 - accuracy: 0.9143\n",
            "Epoch 93/200\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.2145 - accuracy: 0.9619\n",
            "Epoch 94/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.4114 - accuracy: 0.8381\n",
            "Epoch 95/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.2633 - accuracy: 0.9333\n",
            "Epoch 96/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1554 - accuracy: 0.9429\n",
            "Epoch 97/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2231 - accuracy: 0.9524\n",
            "Epoch 98/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.3684 - accuracy: 0.8571\n",
            "Epoch 99/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3294 - accuracy: 0.8952\n",
            "Epoch 100/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.2750 - accuracy: 0.9143\n",
            "Epoch 101/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3946 - accuracy: 0.8762\n",
            "Epoch 102/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3030 - accuracy: 0.9143\n",
            "Epoch 103/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3309 - accuracy: 0.8571\n",
            "Epoch 104/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8762\n",
            "Epoch 105/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4072 - accuracy: 0.8857\n",
            "Epoch 106/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.3525 - accuracy: 0.9143\n",
            "Epoch 107/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2422 - accuracy: 0.9333\n",
            "Epoch 108/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.9238\n",
            "Epoch 109/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3872 - accuracy: 0.8667\n",
            "Epoch 110/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2435 - accuracy: 0.9048\n",
            "Epoch 111/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3262 - accuracy: 0.8857\n",
            "Epoch 112/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.9238\n",
            "Epoch 113/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.2436 - accuracy: 0.9238\n",
            "Epoch 114/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3028 - accuracy: 0.8857\n",
            "Epoch 115/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2220 - accuracy: 0.9524\n",
            "Epoch 116/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.3163 - accuracy: 0.9238\n",
            "Epoch 117/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2409 - accuracy: 0.9333\n",
            "Epoch 118/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.2566 - accuracy: 0.9333\n",
            "Epoch 119/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.3111 - accuracy: 0.9143\n",
            "Epoch 120/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3115 - accuracy: 0.8952\n",
            "Epoch 121/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4158 - accuracy: 0.8762\n",
            "Epoch 122/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.3652 - accuracy: 0.9048\n",
            "Epoch 123/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.2420 - accuracy: 0.9143\n",
            "Epoch 124/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2359 - accuracy: 0.9143\n",
            "Epoch 125/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.3389 - accuracy: 0.8286\n",
            "Epoch 126/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2336 - accuracy: 0.9429\n",
            "Epoch 127/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.3200 - accuracy: 0.9143\n",
            "Epoch 128/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2625 - accuracy: 0.8952\n",
            "Epoch 129/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.3399 - accuracy: 0.8952\n",
            "Epoch 130/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.2493 - accuracy: 0.9048\n",
            "Epoch 131/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2498 - accuracy: 0.9238\n",
            "Epoch 132/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.9143\n",
            "Epoch 133/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.8952\n",
            "Epoch 134/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1743 - accuracy: 0.9429\n",
            "Epoch 135/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.3061 - accuracy: 0.8952\n",
            "Epoch 136/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2880 - accuracy: 0.9143\n",
            "Epoch 137/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.1554 - accuracy: 0.9429\n",
            "Epoch 138/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0905 - accuracy: 0.9810\n",
            "Epoch 139/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.2222 - accuracy: 0.9143\n",
            "Epoch 140/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.1649 - accuracy: 0.9619\n",
            "Epoch 141/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2036 - accuracy: 0.9333\n",
            "Epoch 142/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.1594 - accuracy: 0.9429\n",
            "Epoch 143/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2042 - accuracy: 0.9429\n",
            "Epoch 144/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2174 - accuracy: 0.9333\n",
            "Epoch 145/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1935 - accuracy: 0.9333\n",
            "Epoch 146/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.2398 - accuracy: 0.9143\n",
            "Epoch 147/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.1858 - accuracy: 0.9714\n",
            "Epoch 148/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3544 - accuracy: 0.8381\n",
            "Epoch 149/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.2650 - accuracy: 0.9143\n",
            "Epoch 150/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2653 - accuracy: 0.9048\n",
            "Epoch 151/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2050 - accuracy: 0.9143\n",
            "Epoch 152/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.9333\n",
            "Epoch 153/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.3327 - accuracy: 0.9333\n",
            "Epoch 154/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.2326 - accuracy: 0.9524\n",
            "Epoch 155/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.1860 - accuracy: 0.9333\n",
            "Epoch 156/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2640 - accuracy: 0.9048\n",
            "Epoch 157/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2223 - accuracy: 0.9333\n",
            "Epoch 158/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.1964 - accuracy: 0.9238\n",
            "Epoch 159/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.1502 - accuracy: 0.9524\n",
            "Epoch 160/200\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.2887 - accuracy: 0.9048\n",
            "Epoch 161/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 0.8952\n",
            "Epoch 162/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1978 - accuracy: 0.9333\n",
            "Epoch 163/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.2115 - accuracy: 0.9238\n",
            "Epoch 164/200\n",
            "21/21 [==============================] - 0s 10ms/step - loss: 0.2473 - accuracy: 0.9048\n",
            "Epoch 165/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2160 - accuracy: 0.9524\n",
            "Epoch 166/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.1855 - accuracy: 0.9238\n",
            "Epoch 167/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2654 - accuracy: 0.9143\n",
            "Epoch 168/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.1590 - accuracy: 0.9429\n",
            "Epoch 169/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2072 - accuracy: 0.9333\n",
            "Epoch 170/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.1292 - accuracy: 0.9619\n",
            "Epoch 171/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.1130 - accuracy: 0.9810\n",
            "Epoch 172/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.2301 - accuracy: 0.9238\n",
            "Epoch 173/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9714\n",
            "Epoch 174/200\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.2541 - accuracy: 0.9238\n",
            "Epoch 175/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.1258 - accuracy: 0.9524\n",
            "Epoch 176/200\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.2893 - accuracy: 0.9048\n",
            "Epoch 177/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.1631 - accuracy: 0.9524\n",
            "Epoch 178/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2102 - accuracy: 0.9524\n",
            "Epoch 179/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.1073 - accuracy: 0.9810\n",
            "Epoch 180/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.2082 - accuracy: 0.9238\n",
            "Epoch 181/200\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.1806 - accuracy: 0.9619\n",
            "Epoch 182/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2730 - accuracy: 0.8952\n",
            "Epoch 183/200\n",
            "21/21 [==============================] - 0s 10ms/step - loss: 0.1842 - accuracy: 0.9333\n",
            "Epoch 184/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2552 - accuracy: 0.9143\n",
            "Epoch 185/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1718 - accuracy: 0.9238\n",
            "Epoch 186/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1796 - accuracy: 0.9524\n",
            "Epoch 187/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1692 - accuracy: 0.9524\n",
            "Epoch 188/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.2002 - accuracy: 0.9524\n",
            "Epoch 189/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.1284 - accuracy: 0.9429\n",
            "Epoch 190/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3250 - accuracy: 0.8857\n",
            "Epoch 191/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1552 - accuracy: 0.9524\n",
            "Epoch 192/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.1952 - accuracy: 0.9333\n",
            "Epoch 193/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.1727 - accuracy: 0.9429\n",
            "Epoch 194/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2151 - accuracy: 0.9333\n",
            "Epoch 195/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.1711 - accuracy: 0.9429\n",
            "Epoch 196/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2008 - accuracy: 0.9238\n",
            "Epoch 197/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.1750 - accuracy: 0.9619\n",
            "Epoch 198/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.1525 - accuracy: 0.9619\n",
            "Epoch 199/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.1079 - accuracy: 0.9238\n",
            "Epoch 200/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.1397 - accuracy: 0.9714\n",
            "model created\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Creating training model and saving it\n",
        "'''\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "print(\"model created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ovQFHhfmbVOR"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "def bow(sentence, words, show_details=True):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0]*len(words) \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "'''\n",
        "Filtering and sorting predictions by probability\n",
        "'''\n",
        "def predict_class(sentence, model):\n",
        "    p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Oe4VNUY4bfb8"
      },
      "outputs": [],
      "source": [
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "def chatbot_response(text):\n",
        "    ints = predict_class(text, model)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sp-zY9EWc9m",
        "outputId": "560bfcb7-9d9c-4451-affd-a714dfc537cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for english enter \"eng\", for polish enter \"pol\"pol\n",
            "wybrano pol\n",
            "Jestem prostym botem i mam ograniczony zasob tematow :(\n",
            "Wybierz sposrod ponizszych tematow\n",
            "zwierzeta, manga, warzywa, muzyka, gry\n",
            "wpisz \"quit\" by zakonczyc rozmowe\n",
            "\n",
            "wybor: zwierzeta\n",
            "wybrano zwierzeta,by zmienic wpisz \"category\"\n",
            "Jakie zwierzeta interesuje ciebie psy\n",
            "Chcesz zapytac o psy? (jesli nie to wpisz cokolwiek by wrocic do menu wyzej)\n",
            "page\n",
            "Pytaj!, by zmienic temat, wpisz \"topic\"\n",
            "co to psy\n",
            "Wykorzystywane są jako:\n",
            "\n",
            "psy myśliwskie\n",
            "tropowce (bloodhound)\n",
            "legawce (pudelpointer)\n",
            "płochacze (płochacz niemiecki)\n",
            "posokowce (posokowiec bawarski)\n",
            "gończe (gończy Hamiltona)\n",
            "norowce (jamnik)\n",
            "psy aportujące (curly coated retriever)\n",
            "psy ratownicze (nowofundland)\n",
            "psy pasterskie (anatolian)\n",
            "psy zaganiające (bouvier des Flandres)\n",
            "psy stróżujące (mastif tybetański)\n",
            "psy stróżująco-obronne (doberman)\n",
            "psy służbowe wykorzystywane na potrzeby wojska (owczarek belgijski, czarny terier rosyjski), policji (owczarek niemiecki), służb celnych (pies chodzki)\n",
            "psy zaprzęgowe (husky syberyjski)\n",
            "psy wyścigowe (greyhound)\n",
            "psy reprezentacyjne i do towarzystwa (pudel miniaturowy)\n",
            "psy hodowane w celach konsumpcyjnych (nagi pies meksykański)\n",
            "psy wiejskie (hovawart)\n",
            "psy hodowane w celach terapeutycznych (labrador retriever)\n",
            "\n",
            "\n",
            "== Choroby genetyczne psów ==\n",
            "Choroby genetyczne przenoszone są z pokolenia na pokolenie na zasadzie różnych schematów.\n",
            "Pytaj!, by zmienic temat, wpisz \"topic\"\n",
            "topic\n",
            "Chcesz zapytac o psy? (jesli nie to wpisz cokolwiek by wrocic do menu wyzej)dsg\n",
            "wybrano zwierzeta,by zmienic wpisz \"category\"\n",
            "Jakie zwierzeta interesuje ciebie koty\n",
            "Chcesz zapytac o koty? (jesli nie to wpisz cokolwiek by wrocic do menu wyzej)\n",
            "page\n",
            "Pytaj!, by zmienic temat, wpisz \"topic\"\n",
            "co to koty\n",
            "44 koty (org.\n",
            "Pytaj!, by zmienic temat, wpisz \"topic\"\n",
            "jak sie nazywasz?\n",
            "Co??\n",
            "Pytaj!, by zmienic temat, wpisz \"topic\"\n",
            "imie!\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Co??\n",
            "Pytaj!, by zmienic temat, wpisz \"topic\"\n",
            "topic\n",
            "Chcesz zapytac o koty? (jesli nie to wpisz cokolwiek by wrocic do menu wyzej)dsg\n",
            "wybrano zwierzeta,by zmienic wpisz \"category\"\n",
            "Jakie zwierzeta interesuje ciebie category\n",
            "for english enter \"eng\", for polish enter \"pol\"eng\n",
            "eng selected\n",
            "Im simple bot, and can only talk about selected topics :(\n",
            "What do you want to talk about? chose and type from :\n",
            "animals, manga, vegetables, music, games \n",
            "type \"quit\" to end conversation\n",
            "chose: animals\n",
            "animals chosen, if you want to change category and language, type \"category\"\n",
            "What animals interests you? dogs\n",
            "You want to ask about dogs? (If not type anything to change it)\n",
            "Ask away!, if you want to change topic, type \"topic\"\n",
            "What are dogs?\n",
            "Little is known about these dogs, or the dogs in developed countries that are feral, strays, or are in shelters because the great majority of modern research on dog cognition has focused on pet dogs living in human homes.\n",
            "Ask away!, if you want to change topic, type \"topic\"\n",
            "Do you like animals?\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "I like them very much!\n",
            "Ask away!, if you want to change topic, type \"topic\"\n",
            "Whats is your name?\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "I'm Botti\n",
            "Ask away!, if you want to change topic, type \"topic\"\n",
            "topic\n",
            "You want to ask about dogs? (If not type anything to change it)ewdg\n",
            "animals chosen, if you want to change category and language, type \"category\"\n",
            "What animals interests you? cats\n",
            "You want to ask about cats? (If not type anything to change it)\n",
            "Ask away!, if you want to change topic, type \"topic\"\n",
            "Do you like cats?\n",
            "\"They do that when they're absolutely comfortable with you, and they do it with other cats as well.\"\n",
            "Ask away!, if you want to change topic, type \"topic\"\n",
            "Can you reccomend me an animal?\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "You can watch Mirai Nikki\n",
            "Ask away!, if you want to change topic, type \"topic\"\n",
            "what animal should i adopt?\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Answer is a cat!\n",
            "Ask away!, if you want to change topic, type \"topic\"\n",
            "thank you!\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "My pleasure\n",
            "Ask away!, if you want to change topic, type \"topic\"\n",
            "topic\n",
            "You want to ask about cats? (If not type anything to change it)ddewfsvv\n",
            "animals chosen, if you want to change category and language, type \"category\"\n",
            "What animals interests you? category\n",
            "for english enter \"eng\", for polish enter \"pol\"eng\\\n",
            "error\n",
            "for english enter \"eng\", for polish enter \"pol\"eng\n",
            "eng selected\n",
            "Im simple bot, and can only talk about selected topics :(\n",
            "What do you want to talk about? chose and type from :\n",
            "animals, manga, vegetables, music, games \n",
            "type \"quit\" to end conversation\n",
            "chose: quit\n",
            "bye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    langg = None\n",
        "    while langg == None:\n",
        "      langg = input('for english enter \"eng\", for polish enter \"pol\"').lower()\n",
        "      if langg == 'pol':\n",
        "        print('wybrano pol')\n",
        "        wikipedia.set_lang(\"pl\")\n",
        "        break\n",
        "      elif langg == 'eng':\n",
        "        print('eng selected')\n",
        "        wikipedia.set_lang(\"en\")\n",
        "        break\n",
        "      else:\n",
        "        print(\"error\")\n",
        "        langg = None\n",
        "\n",
        "    if langg == 'eng':\n",
        "      print('Im simple bot, and can only talk about selected topics :(\\n'\n",
        "            'What do you want to talk about? '\n",
        "            'chose and type from :\\n'\n",
        "            'animals, manga, vegetables, music, games \\n'\n",
        "            'type \"quit\" to end conversation')\n",
        "      category = input('chose: ').lower()\n",
        "    else:\n",
        "      print('Jestem prostym botem i mam ograniczony zasob tematow :(\\n'\n",
        "            'Wybierz sposrod ponizszych tematow\\n'\n",
        "            'zwierzeta, manga, warzywa, muzyka, gry\\n'\n",
        "            'wpisz \"quit\" by zakonczyc rozmowe\\n')\n",
        "      category = input('wybor: ').lower()\n",
        "\n",
        "    if category == 'animals' or category == 'zwierzeta' or category == 'manga' or category == 'vegetables' or category == 'warzywa' or category == 'music' or category == 'muzyka' or category == 'games' or category == 'gry':\n",
        "            while True:\n",
        "                if langg == 'eng':\n",
        "                  print(f'{category} chosen, if you want to change category and language, type \"category\"')\n",
        "                  talking = input(f'What {category} interests you? ').lower()\n",
        "                else:\n",
        "                  print(f'wybrano {category},by zmienic wpisz \"category\"')\n",
        "                  talking = input(f'Jakie {category} interesuje ciebie ').lower()\n",
        "                if talking == 'category':\n",
        "                    break\n",
        "                else:\n",
        "                    while True:\n",
        "                      if langg == 'eng':\n",
        "                        question = input(f'You want to ask about {talking}? (If not type anything to change it)').lower()\n",
        "                      else:\n",
        "                        question = input(f'Chcesz zapytac o {talking}? (jesli nie to wpisz cokolwiek by wrocic do menu wyzej)').lower()\n",
        "                      if question:\n",
        "                          break\n",
        "                      else:\n",
        "                          page = (category + ' ' + talking)\n",
        "                          try:\n",
        "                              page = wikipedia.page(page)\n",
        "                          except wikipedia.DisambiguationError as e:\n",
        "                              error_page = random.choice(e.options)\n",
        "                              page = random.choice(error_page)\n",
        "                          except wikipedia.PageError:\n",
        "                              error_page = wikipedia.search(talking)\n",
        "                              save = random.choice(error_page)\n",
        "                              page = wikipedia.summary(save)\n",
        "                              text = wikipedia.page(save).content\n",
        "                          try:\n",
        "                            text = wikipedia.page(page).content\n",
        "                          except wikipedia.WikipediaException:\n",
        "                            page = wikipedia.page(talking)\n",
        "\n",
        "                          while True:\n",
        "                              if langg == 'eng':\n",
        "                                question2 = input('Ask away!, if you want to change topic, type \"topic\"\\n')\n",
        "                              else:\n",
        "                                question2 = input('Pytaj!, by zmienic temat, wpisz \"topic\"\\n')\n",
        "                              output = process(text, question2)\n",
        "                              if question2 == 'topic':\n",
        "                                  break\n",
        "                              elif output:\n",
        "                                  print(output)\n",
        "                              else:\n",
        "                                  if langg == 'eng':\n",
        "                                    message = question2\n",
        "                                    ints = predict_class(message, model)\n",
        "                                    res = getResponse(ints, intents)\n",
        "                                    try:\n",
        "                                          ints = predict_class(message, model)\n",
        "                                          res = getResponse(ints, intents)\n",
        "                                          print(res)\n",
        "                                    except IndexError:\n",
        "                                          print(\"What?\")\n",
        "                                    except RuntimeError:\n",
        "                                          print(\"What??\")\n",
        "                                  else:\n",
        "                                    try:\n",
        "                                        message = translate_eng(question2)\n",
        "                                        ints = predict_class(message, model)\n",
        "                                        res = getResponse(ints, intents)\n",
        "                                        ints = predict_class(message, model)\n",
        "                                        res = getResponse(ints, intents)\n",
        "                                        print(translate(res))\n",
        "                                    except IndexError:\n",
        "                                          print(\"Co?\")\n",
        "                                    except RuntimeError:\n",
        "                                          print(\"Co??\")\n",
        "\n",
        "\n",
        "\n",
        "    elif category == 'quit':\n",
        "        print(\"bye!\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"wrong choice!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}